# -*- coding: utf-8 -*-
"""topic_modelling_app.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NubymPJgA55hGDRwAvjghagW1uHTvsRS

Om Muruga
"""

# LangChain
import os
import streamlit as st
import re
from langchain.schema.document import Document
from langchain.chat_models import ChatOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains.summarize import load_summarize_chain
from langchain.chains import create_extraction_chain
from langchain.document_loaders import YoutubeLoader
from langchain.prompts.chat import (
                        ChatPromptTemplate,
                        SystemMessagePromptTemplate,
                        HumanMessagePromptTemplate)

# LLM chain

def load_youtube_video(youtube_video):
    loader = YoutubeLoader.from_youtube_url(youtube_video, add_video_info=True)
    transcripts = loader.load()
    return transcripts

# Extracting topic title

system_template = 'I want you to act as a Life Coach that can create good titles having only 1 to 2 words!'
system_message_prompt = SystemMessagePromptTemplate.from_template(system_template) # for setting context
human_template = "Title: {text}"
human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)
chat_prompt = ChatPromptTemplate.from_messages(messages=[system_message_prompt, human_message_prompt])

# Splitting the documents
text_splitter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=0)

# llm chain
llm1 = ChatOpenAI(temperature=1)
#llm2 = ChatOpenAI(temperature=1)
chain1 = load_summarize_chain(llm1, chain_type="map_reduce", map_prompt=chat_prompt, verbose=False)
#chain2 = load_summarize_chain(llm2, chain_type="map_reduce", map_prompt=chat_prompt, verbose=False)

# Page Configuration

st.set_page_config(page_title="Topic Modeler",
                            layout="centered",
                            initial_sidebar_state="expanded",
                            )

# Header

st.header("Topic Modeler")

# Columns

col1, col2 = st.columns(2, gap="small")
youtube_video=""
with col1:
    st.markdown("""Extracts topics from video/audio using LLMs.. \n\n*Powered by OpenAI gpt4*"""
                        )

with col2:
    youtube_video = st.text_input(label="Youtube URL", placeholder="Enter youtube url...")
    if youtube_video:
        st.video(youtube_video)

with col1:
    if len(youtube_video) > 0:
        with st.spinner(text='Please Wait...'):
            transcripts = load_youtube_video(youtube_video)
            transcripts[0].page_content = re.sub(r'\[.*?\]\s?', '', transcripts[0].page_content)

            transcript_summary = chain1.run({"input_documents": transcripts})
            #transcript_summary_doc=[]
            #transcript_summary_doc = [Document(page_content=transcript_summary, metadata={"source": "local"})]

            #topic = chain2.run({"input_documents": transcript_summary_doc}) # input: list of Document objects, not a single Document object.
            st.markdown("**Topic of the video**")
            st.write(transcript_summary)